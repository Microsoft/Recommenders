{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR with Spark and SQL\n",
    "\n",
    "Re-implementation of SAR reference Python implementation for Spark (distributed) with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "TOP_K=2\n",
    "RECOMMEND_SEEN=True\n",
    "# options are 'jaccard', 'lift' or '' to skip and use item cooccurrence directly\n",
    "SIMILARITY='jaccard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample dataset - feel free to include the large distributed dataset here - these are just here for testing/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two versions of the dataframes - the numeric version and the alphanumeric one:\n",
    "# they both have similar test data for top-2 recommendations and illustrate the indexing approaches to matrix multiplication on SQL\n",
    "import pandas as pd\n",
    "d_train = {\n",
    "'customerID': [1,1,1,2,2,3,3],\n",
    "'itemID':     [1,2,3,4,5,6,1],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_test = {\n",
    "'customerID': [1,1,2,2,3,3],\n",
    "'itemID':     [4,5,1,5,6,1],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "pdf_test = pd.DataFrame(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 5 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [5 0 0 0 0 3]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_train = np.array([[5,5,5,0,0,0],\\\n",
    "                    [0,0,0,1,1,0],\n",
    "                    [5,0,0,0,0,3]])\n",
    "print(a_train)\n",
    "print(a_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  itemID  rating\n",
       "0           1       4       1\n",
       "1           1       5       1\n",
       "2           2       1       5\n",
       "3           2       5       5\n",
       "4           3       6       5\n",
       "5           3       1       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_alnum_train = {\n",
    "'customerID': ['ua','ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['ia','ib','ic','id','ie','if','ia'],\n",
    "'rating':     [5,5,5,1,1,3,5]\n",
    "}\n",
    "#pdf_train = pd.DataFrame(d_alnum_train)\n",
    "pdf_train = pd.DataFrame(d_train)\n",
    "d_alnum_test = {\n",
    "'customerID': ['ua','ua','ub','ub','uc','uc'],\n",
    "'itemID':     ['id','ie','ia','ie','if','ia'],\n",
    "'rating':     [1,1,5,5,5,5]\n",
    "}\n",
    "#pdf_test = pd.DataFrame(d_alnum_test)\n",
    "pdf_test = pd.DataFrame(d_test)\n",
    "pdf_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ua</td>\n",
       "      <td>ia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ua</td>\n",
       "      <td>ib</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ua</td>\n",
       "      <td>ic</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ub</td>\n",
       "      <td>id</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ub</td>\n",
       "      <td>ie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uc</td>\n",
       "      <td>if</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uc</td>\n",
       "      <td>ia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerID itemID  rating\n",
       "0         ua     ia       5\n",
       "1         ua     ib       5\n",
       "2         ua     ic       5\n",
       "3         ub     id       1\n",
       "4         ub     ie       1\n",
       "5         uc     if       3\n",
       "6         uc     ia       5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Simple\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+----+\n",
      "|customerID|itemID|rating|type|\n",
      "+----------+------+------+----+\n",
      "|         1|     1|     5|   1|\n",
      "|         1|     2|     5|   1|\n",
      "|         1|     3|     5|   1|\n",
      "|         2|     4|     1|   1|\n",
      "|         2|     5|     1|   1|\n",
      "|         3|     6|     3|   1|\n",
      "|         3|     1|     5|   1|\n",
      "+----------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = spark.createDataFrame(pdf_train).withColumn(\"type\", F.lit(1))\n",
    "df_test = spark.createDataFrame(pdf_test).withColumn(\"type\", F.lit(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the user and item IDs\n",
    "\n",
    "Map user and item alphanumeric IDs to matrix indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+----+\n",
      "|customerid|row_id|itemid|col_id|rating|type|\n",
      "+----------+------+------+------+------+----+\n",
      "|         1|     1|     1|     1|     5|   1|\n",
      "|         1|     1|     2|     2|     5|   1|\n",
      "|         1|     1|     3|     3|     5|   1|\n",
      "|         1|     1|     4|     4|     1|   0|\n",
      "|         1|     1|     5|     5|     1|   0|\n",
      "|         2|     2|     1|     1|     5|   0|\n",
      "|         2|     2|     4|     4|     1|   1|\n",
      "|         2|     2|     5|     5|     1|   1|\n",
      "|         2|     2|     5|     5|     5|   0|\n",
      "|         3|     3|     1|     1|     5|   1|\n",
      "|         3|     3|     1|     1|     5|   0|\n",
      "|         3|     3|     6|     6|     3|   1|\n",
      "|         3|     3|     6|     6|     5|   0|\n",
      "+----------+------+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_train = df.count()\n",
    "df_all = df.union(df_test)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "query = \"\"\"\n",
    "SELECT customerid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY customerid) AS row_id,\n",
    "       itemid,\n",
    "       Dense_rank()\n",
    "         OVER(\n",
    "           partition BY 1\n",
    "           ORDER BY itemid)     AS col_id,\n",
    "       rating,\n",
    "       type\n",
    "FROM   df_all \n",
    "\"\"\"\n",
    "df_all = spark.sql(query)\n",
    "df_all.createOrReplaceTempView(\"df_all\")\n",
    "customer_index2ID = dict(df_all.select([\"row_id\", \"customerID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "item_index2ID = dict(df_all.select([\"col_id\", \"itemID\"]).rdd.reduceByKey(lambda k, v: v).collect())\n",
    "df_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     1|     2|     5|\n",
      "|     1|     3|     5|\n",
      "|     2|     4|     1|\n",
      "|     2|     5|     1|\n",
      "|     3|     1|     5|\n",
      "|     3|     6|     3|\n",
      "+------+------+------+\n",
      "\n",
      "+------+------+------+\n",
      "|row_id|col_id|rating|\n",
      "+------+------+------+\n",
      "|     1|     1|     5|\n",
      "|     2|     1|     5|\n",
      "|     3|     1|     5|\n",
      "|     4|     2|     1|\n",
      "|     5|     2|     1|\n",
      "|     1|     3|     5|\n",
      "|     6|     3|     3|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "query = \"\"\"\n",
    "SELECT row_id,\n",
    "       col_id,\n",
    "       rating\n",
    "FROM   df_all\n",
    "WHERE  type = 1 \n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView(\"df_train\")\n",
    "df.show()\n",
    "df_transpose = spark.sql(\"select col_id as row_id, row_id as col_id, rating from df_train\")\n",
    "df_transpose.createOrReplaceTempView(\"df_train_transpose\")\n",
    "df_transpose.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          6|          1|    1|\n",
      "|          3|          1|    1|\n",
      "|          2|          2|    1|\n",
      "|          2|          3|    1|\n",
      "|          1|          2|    1|\n",
      "|          1|          1|    2|\n",
      "|          1|          3|    1|\n",
      "|          5|          4|    1|\n",
      "|          3|          3|    1|\n",
      "|          2|          1|    1|\n",
      "|          3|          2|    1|\n",
      "|          4|          4|    1|\n",
      "|          6|          6|    1|\n",
      "|          1|          6|    1|\n",
      "|          4|          5|    1|\n",
      "|          5|          5|    1|\n",
      "+-----------+-----------+-----+\n",
      "\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id AS row_item_id,\n",
    "       B.col_id AS col_item_id,\n",
    "       Sum(1)   AS value\n",
    "FROM   df_train_transpose A\n",
    "       INNER JOIN df_train B\n",
    "               ON A.col_id = B.row_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_id\n",
    "\"\"\"\n",
    "item_cooccurrence = spark.sql(query)\n",
    "item_cooccurrence.createOrReplaceTempView(\"item_cooccurrence\")\n",
    "item_cooccurrence.show()\n",
    "print(item_cooccurrence.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 0 0 1]\n",
      " [1 1 1 0 0 0]\n",
      " [1 1 1 0 0 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [1 0 0 0 0 1]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "indicator = a_train.copy()\n",
    "indicator[indicator>0]=1\n",
    "item_cooccurrence = indicator.T.dot(indicator)\n",
    "print (item_cooccurrence)\n",
    "print ((item_cooccurrence>0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+\n",
      "|customerID|itemID|rating|\n",
      "+----------+------+------+\n",
      "|        ua|    ia|     5|\n",
      "|        ua|    ib|     5|\n",
      "|        ua|    ic|     5|\n",
      "|        ub|    id|     1|\n",
      "|        ub|    ie|     1|\n",
      "|        uc|    if|     3|\n",
      "|        uc|    ia|     5|\n",
      "+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_train = pd.DataFrame(d_alnum_train)\n",
    "pdf_test = pd.DataFrame(d_alnum_test)\n",
    "\n",
    "df = spark.createDataFrame(pdf_train)\n",
    "df_test = spark.createDataFrame(pdf_test)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| i1| i2|value|\n",
      "+---+---+-----+\n",
      "| ia| if|    1|\n",
      "| ib| ib|    1|\n",
      "| ie| ie|    1|\n",
      "| ia| ib|    1|\n",
      "| ia| ic|    1|\n",
      "| if| if|    1|\n",
      "| ia| ia|    2|\n",
      "| ib| ic|    1|\n",
      "| ic| ic|    1|\n",
      "| id| ie|    1|\n",
      "| id| id|    1|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.itemID i1, B.itemID i2, count(*) value\n",
    "FROM   df2 A INNER JOIN df2 B\n",
    "       ON A.customerID = B.customerID AND \n",
    "          A.itemID <= b.itemID  \n",
    "GROUP  BY A.itemID, B.itemID\n",
    "\"\"\"\n",
    "df.createOrReplaceTempView(\"df2\")\n",
    "item_cooccurrence2 = spark.sql(query)\n",
    "item_cooccurrence2.createOrReplaceTempView(\"item_cooccurrence2\")\n",
    "item_cooccurrence2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  i|margin|\n",
      "+---+------+\n",
      "| ib|     1|\n",
      "| ie|     1|\n",
      "| if|     1|\n",
      "| ia|     2|\n",
      "| ic|     1|\n",
      "| id|     1|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_marginal = spark.sql(\"SELECT i1 i, value AS margin FROM item_cooccurrence2 WHERE i1 = i2\")\n",
    "item_marginal.createOrReplaceTempView(\"item_marginal\")\n",
    "item_marginal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| i1| i2|value|\n",
      "+---+---+-----+\n",
      "| ic| ic|  1.0|\n",
      "| ib| ic|  1.0|\n",
      "| ia| ic|  0.5|\n",
      "| ib| ib|  1.0|\n",
      "| ia| ib|  0.5|\n",
      "| ia| ia|  1.0|\n",
      "| ia| if|  0.5|\n",
      "| if| if|  1.0|\n",
      "| ie| ie|  1.0|\n",
      "| id| ie|  1.0|\n",
      "| id| id|  1.0|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT i1, i2, value / (M1.margin + M2.margin - value) AS value\n",
    "FROM item_cooccurrence2 A \n",
    "    INNER JOIN item_marginal M1 ON A.i1 = M1.i \n",
    "    INNER JOIN item_marginal M2 ON A.i2 = M2.i\n",
    "\"\"\"\n",
    "\n",
    "jaccard2 = spark.sql(query)\n",
    "jaccard2.createOrReplaceTempView(\"jaccard2\")\n",
    "jaccard2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|customerID|itemID|score|\n",
      "+----------+------+-----+\n",
      "|        uc|    ic|  2.5|\n",
      "|        uc|    ib|  2.5|\n",
      "|        ua|    ic| 12.5|\n",
      "|        ub|    ie|  2.0|\n",
      "|        ua|    if|  2.5|\n",
      "|        ua|    ib| 12.5|\n",
      "|        ub|    id|  2.0|\n",
      "|        uc|    ia|  6.5|\n",
      "|        ua|    ia| 10.0|\n",
      "|        uc|    if|  5.5|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A1.customerID, S.i2 itemID,\n",
    "   SUM(A1.rating * S.value) AS score\n",
    "FROM df2 A1,\n",
    "     (SELECT J1.i1, J1.i2, J1.value FROM jaccard2 J1 \n",
    "      UNION ALL \n",
    "      SELECT J2.i2 i1, J2.i1 i2, J2.value FROM jaccard2 J2 WHERE J2.i1 <> J2.i2) S\n",
    "WHERE\n",
    " A1.itemID = S.i1\n",
    "GROUP BY A1.customerID, S.i2 \n",
    "\"\"\"\n",
    "scores = spark.sql(query)\n",
    "scores.show()\n",
    "scores.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard\n",
      "[[1.  0.5 0.5 0.  0.  0.5]\n",
      " [0.5 1.  1.  0.  0.  0. ]\n",
      " [0.5 1.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.  1.  0. ]\n",
      " [0.  0.  0.  1.  1.  0. ]\n",
      " [0.5 0.  0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# show to who to compute Jaccard\n",
    "diag = item_cooccurrence.diagonal()\n",
    "diag_rows = np.expand_dims(diag, axis=0)\n",
    "diag_cols = np.expand_dims(diag, axis=1)\n",
    "# this essentially does vstack(diag_rows).T + vstack(diag_rows) - cooccurrence\n",
    "denom = diag_rows + diag_cols - item_cooccurrence\n",
    "jaccard = item_cooccurrence / denom\n",
    "print (\"Jaccard\")\n",
    "print (jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMILARITY is 'jaccard' or SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id AS i,\n",
    "           A.value       AS d\n",
    "    FROM   item_cooccurrence A\n",
    "    WHERE  A.row_item_id = A.col_item_id \n",
    "    \"\"\"\n",
    "    diagonal = spark.sql(query)\n",
    "    diagonal.createOrReplaceTempView(\"diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_item_id|col_item_id|value|\n",
      "+-----------+-----------+-----+\n",
      "|          1|          1|  1.0|\n",
      "|          6|          1|  0.5|\n",
      "|          3|          1|  0.5|\n",
      "|          2|          1|  0.5|\n",
      "|          1|          6|  0.5|\n",
      "|          6|          6|  1.0|\n",
      "|          1|          3|  0.5|\n",
      "|          3|          3|  1.0|\n",
      "|          2|          3|  1.0|\n",
      "|          5|          5|  1.0|\n",
      "|          4|          5|  1.0|\n",
      "|          5|          4|  1.0|\n",
      "|          4|          4|  1.0|\n",
      "|          1|          2|  0.5|\n",
      "|          3|          2|  1.0|\n",
      "|          2|          2|  1.0|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity = None\n",
    "if SIMILARITY is \"jaccard\":\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d + C.d - A.value ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "elif SIMILARITY is 'lift':\n",
    "    query = \"\"\"\n",
    "    SELECT A.row_item_id,\n",
    "           A.col_item_id,\n",
    "           ( A.value / ( B.d * C.d ) ) AS value\n",
    "    FROM   item_cooccurrence AS A,\n",
    "           diagonal AS B,\n",
    "           diagonal AS C\n",
    "    WHERE  A.row_item_id = B.i\n",
    "           AND A.col_item_id = C.i \n",
    "    \"\"\"\n",
    "    similarity = spark.sql(query)\n",
    "else:\n",
    "    similarity = item_cooccurrence\n",
    "similarity.createOrReplaceTempView(\"item_similarity\")\n",
    "similarity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Affinity Scores\n",
    "\n",
    "Multiply User Affinity by the Item Similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|row_user_id|col_item_id|score|\n",
      "+-----------+-----------+-----+\n",
      "|          3|          1|  6.5|\n",
      "|          1|          2| 12.5|\n",
      "|          1|          1| 10.0|\n",
      "|          1|          3| 12.5|\n",
      "|          2|          5|  2.0|\n",
      "|          3|          3|  2.5|\n",
      "|          2|          4|  2.0|\n",
      "|          3|          6|  5.5|\n",
      "|          3|          2|  2.5|\n",
      "|          1|          6|  2.5|\n",
      "+-----------+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT A.row_id                AS row_user_id,\n",
    "       B.col_item_id,\n",
    "       Sum(A.rating * B.value) AS score\n",
    "FROM   df_train A\n",
    "       INNER JOIN item_similarity B\n",
    "               ON A.col_id = B.row_item_id\n",
    "GROUP  BY A.row_id,\n",
    "          B.col_item_id \n",
    "\"\"\"\n",
    "scores = spark.sql(query)\n",
    "scores.show()\n",
    "scores.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Seen Items\n",
    "\n",
    "Optionally remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping seen items\n",
      "+-----------+-----------+------+\n",
      "|row_user_id|col_item_id|rating|\n",
      "+-----------+-----------+------+\n",
      "|          3|          1|   6.5|\n",
      "|          1|          2|  12.5|\n",
      "|          1|          1|  10.0|\n",
      "|          1|          3|  12.5|\n",
      "|          2|          5|   2.0|\n",
      "|          3|          3|   2.5|\n",
      "|          2|          4|   2.0|\n",
      "|          3|          6|   5.5|\n",
      "|          3|          2|   2.5|\n",
      "|          1|          6|   2.5|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not RECOMMEND_SEEN:\n",
    "    print (\"Removing seen items\")\n",
    "    masked_scores = scores\\\n",
    "        .join(df, (scores.row_user_id == df.row_id) & (scores.col_item_id == df.col_id), \"left_outer\")    \n",
    "    masked_scores.show()\n",
    "    # now since training set is smaller, we have nulls under its value column, i.e. item is not in the\n",
    "    # training set\n",
    "    masked_scores = \\\n",
    "        masked_scores.withColumn(\"rating\", F.when(F.col('rating').isNull(), F.col('score')).otherwise(0))\n",
    "else:\n",
    "    print (\"Keeping seen items\")\n",
    "    scores.createOrReplaceTempView(\"scores\")\n",
    "    masked_scores = spark.sql(\"select row_user_id, col_item_id, score as rating from scores\")\n",
    "masked_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Item Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+---+\n",
      "|row_user_id|col_item_id|rating|top|\n",
      "+-----------+-----------+------+---+\n",
      "|          1|          2|  12.5|  1|\n",
      "|          1|          3|  12.5|  2|\n",
      "|          3|          1|   6.5|  1|\n",
      "|          3|          6|   5.5|  2|\n",
      "|          2|          5|   2.0|  1|\n",
      "|          2|          4|   2.0|  2|\n",
      "+-----------+-----------+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(masked_scores[\"row_user_id\"]).orderBy(masked_scores[\"rating\"].desc())\n",
    "#top_scores =\\\n",
    "#    masked_scores.select(\"*\", F.rank().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n",
    "top_scores =\\\n",
    "    masked_scores.select(\"*\", F.row_number().over(window).alias(\"top\")).filter(F.col(\"top\")<=TOP_K)\n",
    "top_scores.show()\n",
    "top_scores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [{
        "cell_type": "markdown",
        "source": ["# Spark Evaluation"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["This notebook uses MMLSpark, please see github for installation details for your platform.\nhttps://github.com/Azure/mmlspark"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["Evaluation with offline metrics is pivotal to assess the quality of a recommender before it goes into production. Usually, evaluation metrics are carefully chosen based on the actual application scenario of a recommendation system. It is hence important to data scientists and AI developers that build recommendation systems to understand how each evaluation metric is calculated and what it is for.\n\nThis notebook deep dives into several commonly used evaluation metrics, and illustrates how these metrics are used in practice. The metrics covered in this notebook are merely for off-line evaluations."],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["## 0 Global settings"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# set the environment path to find Recommenders\nimport sys\nimport os\nsys.path.append(\"../../\")\nimport pandas as pd\nimport pyspark\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfrom mmlspark.RankingAdapter import RankingAdapter\nfrom mmlspark.RankingEvaluator import RankingEvaluator\n\nSUBMIT_ARGS = \"--packages Azure:mmlspark:0.15 pyspark-shell\"\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n\nfrom pyspark.sql import SparkSession\n\nspark = (\n   SparkSession.builder.appName(\"Spark Evaluation Metrics\")\n   .master(\"local[*]\")\n   .config(\"memory\", \"1G\")\n   .getOrCreate()\n)\n\nprint(\"System version: {}\".format(sys.version))\nprint(\"Pandas version: {}\".format(pd.__version__))\nprint(\"PySpark version: {}\".format(pyspark.__version__))"],
        "metadata": {
            "trusted": true
        },
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">System version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n[GCC 5.4.0 20160609]\nPandas version: 0.19.2\nPySpark version: 2.3.0\n</div>"]
            }
        }],
        "execution_count": 5
    }, {
        "cell_type": "markdown",
        "source": ["Note to successfully run Spark codes with the Jupyter kernel, one needs to correctly set the environment variables of `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` that point to Python executables with the desired version. Detailed information can be found in the setup instruction document [SETUP.md](https://raw.githubusercontent.com/dciborow/Recommenders/master/SETUP.md)."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["COL_USER = \"UserId\"\nCOL_ITEM = \"MovieId\"\nCOL_RATING = \"Rating\"\nCOL_PREDICTION = \"prediction\""],
        "metadata": {
            "trusted": true
        },
        "outputs": [],
        "execution_count": 7
    }, {
        "cell_type": "markdown",
        "source": ["## 1 Prepare data"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["For illustration purpose, a dummy data set is created for demonstrating how different evaluation metrics work. \n\nThe data has the schema that can be frequently found in a recommendation problem, that is, each row in the dataset is a (user, item, rating) tuple, where \"rating\" can be an ordinal rating score (e.g., discrete integers of 1, 2, 3, etc.) or an numerical float number that quantitatively indicates the preference of the user towards that item. \n\nFor simplicity reason, the column of rating in the dummy dataset we use in the example represent some ordinal ratings."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["df_true = pd.DataFrame(\n        {\n            COL_USER: [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n            COL_ITEM: [1, 2, 3, 1, 4, 5, 6, 7, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14],\n            COL_RATING: [5, 4, 3, 5, 5, 3, 3, 1, 5, 5, 5, 4, 4, 3, 3, 3, 2, 1],\n        }\n    )\nratings = spark.createDataFrame(df_true)\n\nratings.filter(ratings[COL_USER] == 1).show()"],
        "metadata": {
            "trusted": true
        },
        "outputs": [],
        "execution_count": 10
    }, {
        "cell_type": "markdown",
        "source": ["Build the recommendation model using ALS on the training data.\n\nNote we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["als = ALS(maxIter=5, regParam=0.01, userCol=COL_USER, itemCol=COL_ITEM, ratingCol=COL_RATING,\n          coldStartStrategy=\"drop\")\npredictions = als.fit(ratings).transform(ratings)\n\npredictions.show()"],
        "metadata": {
            "trusted": true
        },
        "outputs": [],
        "execution_count": 12
    }, {
        "cell_type": "markdown",
        "source": ["## 2 Evaluation metrics"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["### 2.1 Rating metrics"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=COL_RATING, predictionCol=\"prediction\").evaluate(predictions)\nprint(\"The RMSE is {}\".format(rmse))\n\nr2 = RegressionEvaluator(metricName=\"r2\", labelCol=COL_RATING, predictionCol=\"prediction\").evaluate(predictions)\nprint(\"The R2 is {}\".format(r2))\n\nmae = RegressionEvaluator(metricName=\"mae\", labelCol=COL_RATING, predictionCol=\"prediction\").evaluate(predictions)\nprint(\"The MAE is {}\".format(mae))\n\nmse = RegressionEvaluator(metricName=\"mse\", labelCol=COL_RATING, predictionCol=\"prediction\").evaluate(predictions)\nprint(\"The mse is {}\".format(mse))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 15
    }, {
        "cell_type": "markdown",
        "source": ["### 2.2 Ranking metrics"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["The following code fits the model, and creates the properly formated output dataset for ranking evaluation."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["output = RankingAdapter(k=5, recommender=als).fit(ratings).transform(ratings)\n\noutput.show()"],
        "metadata": {
            "trusted": true
        },
        "outputs": [],
        "execution_count": 18
    }, {
        "cell_type": "markdown",
        "source": ["A few ranking metrics can then be calculated."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# prec = RankingEvaluator(k=5, metricName='precision').evaluate(output)\n# print(\"The precision at k is {}\".format(prec))\n\n# recall = RankingEvaluator(k=5, metricName='recall').evaluate(output)\n# print(\"The recall at k is {}\".format(recall))\n\nndcg = RankingEvaluator(k=5, metricName='ndcgAt').evaluate(output)\nprint(\"The ndcg at k is {}\".format(ndcg))\n\nmap  = RankingEvaluator(k=5, metricName='map').evaluate(output)\nprint(\"The map at k is {}\".format(map))\n\nfcp  = RankingEvaluator(k=5, metricName='fcp').evaluate(output)\nprint(\"The fcp is {}\".format(fcp))\n\nmrr  = RankingEvaluator(k=5, metricName='mrr').evaluate(output)\nprint(\"The mrr is {}\".format(mrr))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 20
    }],
    "metadata": {
        "kernelspec": {
            "name": "spark-3-python",
            "display_name": "Python 3 Spark - local",
            "language": "python"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "version": "3.5.5",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "name": "evaluation_spark",
        "notebookId": 4328852026479626
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click-prediction with XDeepFM\n",
    "\n",
    "(PROPOSAL: Start with what is the problem we are addressing and why the user should care)\n",
    "\n",
    "In this notebook we are going to analyze an example of collaborative filtering using the Microsoft Research algorithm xDeepFM ([Paper](https://arxiv.org/abs/1803.05170)). For it, we are going to use the [dataset CRITEO](https://www.kaggle.com/c/criteo-display-ad-challenge/data), which contains:\n",
    "\n",
    "- Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    "- I1-I13 - A total of 13 columns of integer features (mostly count features).\n",
    "- C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes. \n",
    "\n",
    "The training set consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo. Positive (clicked) and negatives (non-clicked) examples have both been subsampled at different rates in order to reduce the dataset size. The examples are chronologically ordered. Label - Target variable that indicates if an ad was clicked (1) or not (0).\n",
    "\n",
    "An algorithm like xDeepFM can be beneficial for problems of click optimization, where the objective is to maximize the CTR (TODO: link to CTR info). The evaluation metrics that we are going to use are regression metrics like RMSE, AUC or logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xDeepFM\n",
    "\n",
    "Combinatorial features are essential for the success of many com- mercial models. Manually crafting these features usually\n",
    "comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models,\n",
    "which measure interactions in terms of vector product, can learn patterns of com- binatorial features automatically and \n",
    "generalize to unseen features as well. With the great success of deep neural works (DNNs) in various fields, recently \n",
    "researchers have proposed several DNN- based factorization model to learn both low- and high-order feature interactions.\n",
    "Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions im- \n",
    "plicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims \n",
    "to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some \n",
    "functionalities with con- volutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a \n",
    "CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factor- ization Machine (xDeepFM). \n",
    "On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it\n",
    "can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three\n",
    "real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_rows = 1000000\n",
    "data_url = \"https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n",
      "Numpy:  1.15.1\n",
      "Number of CPU processors:  24\n",
      "GPU:  ['TITAN V', 'TITAN V', 'TITAN V']\n",
      "GPU memory:  ['12065 MiB', '12066 MiB', '12066 MiB']\n",
      "CUDA:  CUDA Version 9.1.85\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from machine_utils import (\n",
    "    get_gpu_name,\n",
    "    get_number_processors,\n",
    "    get_gpu_memory,\n",
    "    get_cuda_version,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"xDeepFM/exdeepfm\")\n",
    "\n",
    "import config_utils\n",
    "import utils.util as util\n",
    "import utils.metric as metric\n",
    "import train\n",
    "\n",
    "from train import cache_data, run_eval, run_infer, create_train_model\n",
    "from utils.log import Log\n",
    "from src.exDeepFM import ExtremeDeepFMModel\n",
    "\n",
    "import utilities\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Number of CPU processors: \", get_number_processors())\n",
    "# breaks built on CPU/mac\n",
    "# print(\"GPU: \", get_gpu_name())\n",
    "# print(\"GPU memory: \", get_gpu_memory())\n",
    "# print(\"CUDA: \", get_cuda_version())\n",
    "\n",
    "# runtime checks\n",
    "util.check_tensorflow_version()\n",
    "util.check_and_mkdir()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# TODO\n",
    "# T=4 # cut-off for minimum counts\n",
    "# nrows=10000 # limit the data to reduce runtime\n",
    "\n",
    "# supplied files come without header\n",
    "fieldnames = ['Label', \\\n",
    "             'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13', 'C1', 'C2', 'C3', 'C4', \\\n",
    "             'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', \\\n",
    "             'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset retrieval\n",
    "\n",
    "See [Criteo](https://www.kaggle.com/c/criteo-display-ad-challenge/data) for licencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "urlretrieve(\n",
    "   data_url,\n",
    "   \"dac.tar.gz\",\n",
    "   lambda count, blockSize, totalSize: sys.stdout.write(\n",
    "       \"\\rDownloading...%d%%\" % int(count * blockSize * 100 / totalSize)\n",
    "   )\n",
    ")\n",
    "\n",
    "print(\"\\n\\nExtracing data\")\n",
    "with tarfile.open(\"dac.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"data\")\n",
    "\n",
    "train_head = pd.read_csv(\"data/train.txt\", names=fieldnames, sep=\"\\t\", nrows=5)\n",
    "train_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43591 features\n"
     ]
    }
   ],
   "source": [
    "# create data staging directory\n",
    "utilities.mkdir_safe(\"data_prep\")\n",
    "\n",
    "full_data = \"data_prep/full.txt\"\n",
    "full_data_ffm = \"data_prep/full.ffm\"\n",
    "\n",
    "utilities.split_files(\"data/train.txt\", [full_data], [1], max_rows=max_rows)\n",
    "\n",
    "feat_cnt = defaultdict(lambda: 0)\n",
    "\n",
    "for row in csv.DictReader(open(full_data), fieldnames=fieldnames, delimiter=\"\\t\"):\n",
    "    for key, val in row.items():\n",
    "        if \"C\" in key:\n",
    "            if val == \"\":\n",
    "                feat_cnt[str(key) + \"#\" + \"absence\"] += 1\n",
    "            else:\n",
    "                feat_cnt[str(key) + \"#\" + str(val)] += 1\n",
    "\n",
    "print(\"Found %d features\" % len(feat_cnt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "* Handle missing values\n",
    "* Integers > 2: logarithmic transform (discriminate the small values [details](https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf))  \n",
    "* Integers $\\le$ 2: convert to categorical\n",
    "* Categoricals below minimum threshold (=T): replace with categorical floor feature (**column name** # **feature count**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(key, val):\n",
    "    if \"I\" in key and key != \"Id\":\n",
    "        if val == \"\":\n",
    "            # handle missing values\n",
    "            return str(key) + \"#\" + \"absence\"\n",
    "        else:\n",
    "            val = int(val)\n",
    "            if val > 2:\n",
    "                # log transform + ^2 to discriminate small values\n",
    "                val = int(math.log(float(val)) ** 2)\n",
    "            else:\n",
    "                # convert to categorical\n",
    "                val = \"SP\" + str(val)\n",
    "            return str(key) + \"#\" + str(val)\n",
    "\n",
    "        if \"C\" in key:\n",
    "            if val == \"\":\n",
    "                # handle missing values\n",
    "                return str(key) + \"#\" + \"absence\"\n",
    "            else:\n",
    "                return str(key) + \"#\" + str(val)\n",
    "            if feat_cnt[feat] <= T:\n",
    "                # group values with small frequencies\n",
    "                return str(key) + \"#\" + str(feat_cnt[feat])\n",
    "\n",
    "        raise ValueError(\"Unsupported key: '%s'\" % key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       data_prep/full.txt 0:     7818 (78.2%) 1:     2182 (21.8%)\n"
     ]
    }
   ],
   "source": [
    "featSet = set()\n",
    "label_cnt = defaultdict(lambda: 0)\n",
    "\n",
    "for row in csv.DictReader(open(full_data), fieldnames=fieldnames, delimiter=\"\\t\"):\n",
    "    for key, val in row.items():\n",
    "        if key == \"Label\":\n",
    "            label_cnt[str(val)] += 1\n",
    "            continue\n",
    "\n",
    "        feat = get_feature(key, val)\n",
    "        featSet.add(feat)\n",
    "\n",
    "rows = sum(label_cnt.values())\n",
    "print(\n",
    "    \"%25s 0: %8d (%.1f%%) 1: %8d (%.1f%%)\"\n",
    "    % (\n",
    "        full_data,\n",
    "        label_cnt[\"0\"],\n",
    "        label_cnt[\"0\"] * 100 / rows,\n",
    "        label_cnt[\"1\"],\n",
    "        label_cnt[\"1\"] * 100 / rows,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate feature and column statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features count: 710\n",
      "Field count: 39\n"
     ]
    }
   ],
   "source": [
    "featIndex = dict()\n",
    "for index, feat in enumerate(featSet, start=1):\n",
    "    featIndex[feat] = index\n",
    "print(\"Categorical features count:\", len(featIndex))\n",
    "\n",
    "fieldIndex = dict()\n",
    "fieldList = fieldnames[1:]\n",
    "\n",
    "for index, field in enumerate(fieldList, start=1):\n",
    "    fieldIndex[field] = index\n",
    "print(\"Field count:\", len(fieldIndex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to [ffm format](https://github.com/guestwalk/libffm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(full_data_ffm, \"w\") as out:\n",
    "    for row in csv.DictReader(open(full_data), fieldnames=fieldnames, delimiter=\"\\t\"):\n",
    "        feats = []\n",
    "\n",
    "        for key, val in row.items():\n",
    "            if key == \"Label\":\n",
    "                feats.append(val)\n",
    "                continue\n",
    "\n",
    "            feat = get_feature(key, val)\n",
    "            # lookup field index + lookup feature index\n",
    "            feats.append(str(fieldIndex[key]) + \":\" + str(featIndex[feat]) + \":1\")\n",
    "\n",
    "        out.write(\" \".join(feats) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFM Format\n",
    "One example entry \"1:552:1\" can be split into \n",
    "\n",
    "* field: 1\n",
    "* feature index: 552\n",
    "* feature value: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1:352:1</td>\n",
       "      <td>2:151:1</td>\n",
       "      <td>3:167:1</td>\n",
       "      <td>4:455:1</td>\n",
       "      <td>5:273:1</td>\n",
       "      <td>6:535:1</td>\n",
       "      <td>7:646:1</td>\n",
       "      <td>8:111:1</td>\n",
       "      <td>9:38:1</td>\n",
       "      <td>...</td>\n",
       "      <td>30:607:1</td>\n",
       "      <td>31:607:1</td>\n",
       "      <td>32:607:1</td>\n",
       "      <td>33:607:1</td>\n",
       "      <td>34:607:1</td>\n",
       "      <td>35:607:1</td>\n",
       "      <td>36:607:1</td>\n",
       "      <td>37:607:1</td>\n",
       "      <td>38:607:1</td>\n",
       "      <td>39:607:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1:264:1</td>\n",
       "      <td>2:304:1</td>\n",
       "      <td>3:416:1</td>\n",
       "      <td>4:300:1</td>\n",
       "      <td>5:475:1</td>\n",
       "      <td>6:190:1</td>\n",
       "      <td>7:281:1</td>\n",
       "      <td>8:111:1</td>\n",
       "      <td>9:669:1</td>\n",
       "      <td>...</td>\n",
       "      <td>30:607:1</td>\n",
       "      <td>31:607:1</td>\n",
       "      <td>32:607:1</td>\n",
       "      <td>33:607:1</td>\n",
       "      <td>34:607:1</td>\n",
       "      <td>35:607:1</td>\n",
       "      <td>36:607:1</td>\n",
       "      <td>37:607:1</td>\n",
       "      <td>38:607:1</td>\n",
       "      <td>39:607:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1:264:1</td>\n",
       "      <td>2:304:1</td>\n",
       "      <td>3:700:1</td>\n",
       "      <td>4:220:1</td>\n",
       "      <td>5:633:1</td>\n",
       "      <td>6:461:1</td>\n",
       "      <td>7:233:1</td>\n",
       "      <td>8:111:1</td>\n",
       "      <td>9:320:1</td>\n",
       "      <td>...</td>\n",
       "      <td>30:607:1</td>\n",
       "      <td>31:607:1</td>\n",
       "      <td>32:607:1</td>\n",
       "      <td>33:607:1</td>\n",
       "      <td>34:607:1</td>\n",
       "      <td>35:607:1</td>\n",
       "      <td>36:607:1</td>\n",
       "      <td>37:607:1</td>\n",
       "      <td>38:607:1</td>\n",
       "      <td>39:607:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1:133:1</td>\n",
       "      <td>2:394:1</td>\n",
       "      <td>3:21:1</td>\n",
       "      <td>4:32:1</td>\n",
       "      <td>5:464:1</td>\n",
       "      <td>6:53:1</td>\n",
       "      <td>7:63:1</td>\n",
       "      <td>8:359:1</td>\n",
       "      <td>9:351:1</td>\n",
       "      <td>...</td>\n",
       "      <td>30:607:1</td>\n",
       "      <td>31:607:1</td>\n",
       "      <td>32:607:1</td>\n",
       "      <td>33:607:1</td>\n",
       "      <td>34:607:1</td>\n",
       "      <td>35:607:1</td>\n",
       "      <td>36:607:1</td>\n",
       "      <td>37:607:1</td>\n",
       "      <td>38:607:1</td>\n",
       "      <td>39:607:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1:518:1</td>\n",
       "      <td>2:602:1</td>\n",
       "      <td>3:21:1</td>\n",
       "      <td>4:455:1</td>\n",
       "      <td>5:616:1</td>\n",
       "      <td>6:37:1</td>\n",
       "      <td>7:233:1</td>\n",
       "      <td>8:359:1</td>\n",
       "      <td>9:351:1</td>\n",
       "      <td>...</td>\n",
       "      <td>30:607:1</td>\n",
       "      <td>31:607:1</td>\n",
       "      <td>32:607:1</td>\n",
       "      <td>33:607:1</td>\n",
       "      <td>34:607:1</td>\n",
       "      <td>35:607:1</td>\n",
       "      <td>36:607:1</td>\n",
       "      <td>37:607:1</td>\n",
       "      <td>38:607:1</td>\n",
       "      <td>39:607:1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label       I1       I2       I3       I4       I5       I6       I7  \\\n",
       "0      0  1:352:1  2:151:1  3:167:1  4:455:1  5:273:1  6:535:1  7:646:1   \n",
       "1      0  1:264:1  2:304:1  3:416:1  4:300:1  5:475:1  6:190:1  7:281:1   \n",
       "2      0  1:264:1  2:304:1  3:700:1  4:220:1  5:633:1  6:461:1  7:233:1   \n",
       "3      0  1:133:1  2:394:1   3:21:1   4:32:1  5:464:1   6:53:1   7:63:1   \n",
       "4      0  1:518:1  2:602:1   3:21:1  4:455:1  5:616:1   6:37:1  7:233:1   \n",
       "\n",
       "        I8       I9    ...          C17       C18       C19       C20  \\\n",
       "0  8:111:1   9:38:1    ...     30:607:1  31:607:1  32:607:1  33:607:1   \n",
       "1  8:111:1  9:669:1    ...     30:607:1  31:607:1  32:607:1  33:607:1   \n",
       "2  8:111:1  9:320:1    ...     30:607:1  31:607:1  32:607:1  33:607:1   \n",
       "3  8:359:1  9:351:1    ...     30:607:1  31:607:1  32:607:1  33:607:1   \n",
       "4  8:359:1  9:351:1    ...     30:607:1  31:607:1  32:607:1  33:607:1   \n",
       "\n",
       "        C21       C22       C23       C24       C25       C26  \n",
       "0  34:607:1  35:607:1  36:607:1  37:607:1  38:607:1  39:607:1  \n",
       "1  34:607:1  35:607:1  36:607:1  37:607:1  38:607:1  39:607:1  \n",
       "2  34:607:1  35:607:1  36:607:1  37:607:1  38:607:1  39:607:1  \n",
       "3  34:607:1  35:607:1  36:607:1  37:607:1  38:607:1  39:607:1  \n",
       "4  34:607:1  35:607:1  36:607:1  37:607:1  38:607:1  39:607:1  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ffm_head = pd.read_csv(full_data_ffm, names=fieldnames, sep=' ', nrows=5)\n",
    "train_ffm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, test and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_ffm = \"data_prep/train.ffm\"\n",
    "eval_file_ffm = \"data_prep/eval.ffm\"\n",
    "test_file_ffm = \"data_prep/test.ffm\"\n",
    "\n",
    "utilities.split_files(\n",
    "    full_data_ffm, [train_file_ffm, test_file_ffm, eval_file_ffm], [0.8, 0.1, 0.1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training network configuration file is config/exDeepFM.yaml\n"
     ]
    }
   ],
   "source": [
    "# load network hyper-parameter\n",
    "config = config_utils.load_yaml(\"config/exDeepFM.yaml\")\n",
    "\n",
    "# patch config to reflect the current data set\n",
    "config[\"data\"][\"FEATURE_COUNT\"] = len(featIndex)\n",
    "config[\"data\"][\"FIELD_COUNT\"] = len(fieldIndex)\n",
    "\n",
    "config[\"data\"][\"train_file\"] = train_file_ffm\n",
    "config[\"data\"][\"eval_file\"] = eval_file_ffm\n",
    "config[\"data\"][\"test_file\"] = test_file_ffm\n",
    "del config[\"data\"][\"infer_file\"]\n",
    "\n",
    "# setup hparams\n",
    "hparams = config_utils.create_hparams(config)\n",
    "log = Log(hparams)\n",
    "hparams.logger = log.logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 710), ('FIELD_COUNT', 39), ('PAIR_NUM', None), ('activation', ['relu', 'relu', 'relu', 'relu']), ('attention_activation', None), ('attention_layer_sizes', None), ('batch_size', 4096), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', [100, 100, 50]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('dropout', [0.0, 0.0, 0.0, 0.0]), ('embed_l1', 0.0), ('embed_l2', 0.001), ('epochs', 10), ('eval_file', 'data_prep/eval.ffm'), ('infer_file', None), ('init_method', 'tnormal'), ('init_value', 0.1), ('layer_l1', 0.0), ('layer_l2', 0.001), ('layer_sizes', [400, 400, 400, 400]), ('learning_rate', 0.001), ('load_model_name', None), ('log', 'log'), ('logger', <Logger utils.log (INFO)>), ('loss', 'log_loss'), ('method', 'classification'), ('metrics', ['auc', 'logloss']), ('model_type', 'exDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('optimizer', 'adam'), ('save_epoch', 2), ('show_step', 20), ('test_file', 'data_prep/test.ffm'), ('train_file', 'data_prep/train.ffm')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create extreme deep FM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache filename: data_prep/train.ffm\n",
      "has not cached file, begin cached...\n",
      "caced file used time, 2s, Fri Sep 28 14:47:31 2018.\n",
      "data sample num:7943\n",
      "cache filename: data_prep/eval.ffm\n",
      "has not cached file, begin cached...\n",
      "caced file used time, 0s, Fri Sep 28 14:47:31 2018.\n",
      "data sample num:1033\n",
      "cache filename: data_prep/test.ffm\n",
      "has not cached file, begin cached...\n",
      "caced file used time, 0s, Fri Sep 28 14:47:32 2018.\n",
      "data sample num:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/miniconda3/envs/xDeepFM-criteo/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10\n",
      "total_loss = data_loss+regularization_loss, data_loss = logloss\n",
      "\n",
      "Epoch 0\n",
      "Train loss: 4.946; Test loss: 0.530; Eval loss: 0.537 auc: 0.582\n",
      "\n",
      "Epoch 1\n",
      "Train loss: 4.643; Test loss: 0.582; Eval loss: 0.596 auc: 0.596\n",
      "\n",
      "Epoch 2\n",
      "Train loss: 4.613; Test loss: 0.554; Eval loss: 0.565 auc: 0.605\n",
      "\n",
      "Epoch 3\n",
      "Train loss: 4.479; Test loss: 0.506; Eval loss: 0.512 auc: 0.607\n",
      "\n",
      "Epoch 4\n",
      "Train loss: 4.358; Test loss: 0.528; Eval loss: 0.532 auc: 0.610\n",
      "\n",
      "Epoch 5\n",
      "Train loss: 4.289; Test loss: 0.514; Eval loss: 0.519 auc: 0.614\n",
      "\n",
      "Epoch 6\n",
      "Train loss: 4.187; Test loss: 0.502; Eval loss: 0.510 auc: 0.618\n",
      "\n",
      "Epoch 7\n",
      "Train loss: 4.098; Test loss: 0.506; Eval loss: 0.515 auc: 0.621\n",
      "\n",
      "Epoch 8\n",
      "Train loss: 4.016; Test loss: 0.501; Eval loss: 0.510 auc: 0.624\n",
      "\n",
      "Epoch 9\n",
      "Train loss: 3.926; Test loss: 0.499; Eval loss: 0.507 auc: 0.628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache_data(hparams, hparams.train_file, flag='train')\n",
    "cache_data(hparams, hparams.eval_file, flag='eval')\n",
    "cache_data(hparams, hparams.test_file, flag='test')\n",
    "\n",
    "train_model = create_train_model(ExtremeDeepFMModel, hparams)\n",
    "\n",
    "gpuconfig = tf.ConfigProto()\n",
    "gpuconfig.gpu_options.allow_growth = True\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "train_sess = tf.Session(target='', graph=train_model.graph, config=gpuconfig)\n",
    "train_sess.run(train_model.model.init_op)\n",
    "\n",
    "print('Epochs: %d' % hparams.epochs)\n",
    "print('total_loss = data_loss+regularization_loss, data_loss = logloss\\n')\n",
    "\n",
    "with tf.summary.FileWriter(util.SUMMARIES_DIR, train_sess.graph) as writer:\n",
    "    last_eval = 0\n",
    "    \n",
    "    for epoch in range(hparams.epochs):\n",
    "        print('Epoch %d' % epoch)\n",
    "        step = 0\n",
    "        train_sess.run(train_model.iterator.initializer, feed_dict={train_model.filenames: [hparams.train_file_cache]})\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # TODO: collect timing infomration\n",
    "        while True:\n",
    "            try:\n",
    "                # TODO: collect timing information \n",
    "                (_, step_loss, step_data_loss, summary) = train_model.model.train(train_sess)\n",
    "                writer.add_summary(summary, step)\n",
    "                \n",
    "                epoch_loss += step_loss\n",
    "                step += 1\n",
    "                \n",
    "                if step % hparams.show_step == 0:\n",
    "                    print('Step {0:d}: total_loss: {1:.4f} data_loss: {2:.4f}' \\\n",
    "                          .format(step, step_loss, step_data_loss))\n",
    "            except tf.errors.OutOfRangeError as e:\n",
    "                break\n",
    "    \n",
    "        # TODO: do we need  model saving in between?\n",
    "        if epoch % hparams.save_epoch == 0:\n",
    "            checkpoint_path = train_model.model.saver.save(\n",
    "                sess=train_sess,\n",
    "                save_path=util.MODEL_DIR + 'epoch_' + str(epoch))\n",
    "\n",
    "        eval_res = run_eval(train_model, train_sess, hparams.eval_file_cache, util.EVAL_NUM, hparams, flag='eval')\n",
    "        test_res = run_eval(train_model, train_sess, hparams.test_file_cache, util.TEST_NUM, hparams, flag='test')\n",
    "        \n",
    "        print ('Train loss: %1.3f; Test loss: %1.3f; Eval loss: %1.3f auc: %0.3f\\n' \n",
    "              % (epoch_loss / step, test_res['logloss'], eval_res['logloss'], eval_res['auc']))\n",
    "\n",
    "        # early stopping\n",
    "        if eval_res[\"auc\"] - last_eval < - 0.003:\n",
    "            break\n",
    "        if eval_res[\"auc\"] > last_eval:\n",
    "            last_eval = eval_res[\"auc\"]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

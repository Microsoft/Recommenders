{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBT Single Node on MovieLens (Python, CPU)\n",
    "\n",
    "FBT (Frequently Bought together) recommender can be thought of as a even simpler restriction of the SAR (Simple Algorithm for Recommendation). Like SAR, FBT is a fast and scalable algorithm for personalized recommendations based on user transaction history. SAR leverages user ratings of items and timestamp information of when user rated an item to produce easily explainable and interpretable recommendations. But there are many scenarios where we may not have reliable rating information or timestamps. All we have is user interactions with items and we need a simple recommendation engine that can leverage this interaction information without regard to context or quality of interaction or when in history did this interaction happen.\n",
    "\n",
    "This is where we can leverage FBT. Like SAR, FBT recommends items that are most ***similar*** to the ones that the user already has an existing ***affinity*** for. Two items are ***similar*** if the users that interacted with one item are also likely to have interacted with the other. Unlike SAR though, user ***affinity*** to an item is simply binary - 1 if the user has interacted with an item in the past, 0 otherwise.\n",
    "\n",
    "### Advantages of FBT:\n",
    "- A simple first algorithm to implement when all you have is users and items and no more information. Covers a broad range of customer scenarios.\n",
    "- High accuracy for an easy to train and deploy algorithm\n",
    "- Fast training and scoring, only requiring simple counting to construct matrices used at prediction time.\n",
    "\n",
    "### Notes to use FBT properly:\n",
    "- Since FBT uses very little information, recommendations will likely not have more context than historical interactions. If we can leverage useful information from item or user features, more sohisticated algorithms will have an edge in performance.\n",
    "\n",
    "- It's memory-hungry, requiring the creation of an $mxm$ sparse square matrix (where $m$ is the number of items). This can also be a problem for many matrix factorization algorithms.\n",
    "- FBT does not need ratings information, hence we can't predict ratings either. Evaluation can best happen with user studies. We can still look at offline evaluation methods like Precision@K, Recall@K.\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate FBT in Python on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "System version: 3.6.11 | packaged by conda-forge | (default, Nov 27 2020, 18:51:43) \n[GCC Clang 11.0.0]\nPandas version: 1.1.5\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from reco_utils.common.python_utils import binarize\n",
    "from reco_utils.common.timer import Timer\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    map_at_k,\n",
    "    ndcg_at_k,\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    "    rmse,\n",
    "    mae,\n",
    "    logloss,\n",
    "    rsquared,\n",
    "    exp_var\n",
    ")\n",
    "from reco_utils.recommender.sar import SAR\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "FBT is intended to be used on a very simple schema: `<User ID>, <Item ID>.` Each row represents a single interaction between a user and an item. These interactions might be different types of events on an e-commerce website, such as a user clicking to view an item, adding it to a shopping basket, following a recommendation link, and so on. \n",
    "\n",
    "The MovieLens dataset is well formatted interactions of Users providing Ratings to Movies (movie ratings are used as the event weight). We will swap out the movielens ratings information with a dummy rating of 1.0 (if user interacted with an item) in order to leverage SAR signature and to showcase simple FBT with only users and items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download and use the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:03<00:00, 1.27kKB/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userID  itemID         Title  rating\n",
       "0     196     242  Kolya (1996)     1.0\n",
       "1      63     242  Kolya (1996)     1.0\n",
       "2     226     242  Kolya (1996)     1.0\n",
       "3     154     242  Kolya (1996)     1.0\n",
       "4     306     242  Kolya (1996)     1.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>Title</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>Kolya (1996)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63</td>\n      <td>242</td>\n      <td>Kolya (1996)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>226</td>\n      <td>242</td>\n      <td>Kolya (1996)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>154</td>\n      <td>242</td>\n      <td>Kolya (1996)</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>306</td>\n      <td>242</td>\n      <td>Kolya (1996)</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=('userID', 'itemID'),\n",
    "    title_col=\"Title\"\n",
    ")\n",
    "\n",
    "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
    "data['rating'] = np.float32(1.0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split the data using the python random splitter provided in utilities:\n",
    "\n",
    "We split the full dataset into a `train` and `test` dataset to evaluate performance of the algorithm against a held-out set not seen during training. Because FBT generates recommendations based on user preferences, all users that are in the test set must also exist in the training set. For this case, we can use the provided `python_stratified_split` function which holds out a percentage (in this case 25%) of items from each user, but ensures all users are in both `train` and `test` datasets. Other options are available in the `dataset.python_splitters` module which provide more control over how the split occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(data, ratio=0.75, col_user='userID', col_item='itemID', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTrain:\nTotal Ratings: 74992\nUnique Users: 943\nUnique Items: 1601\n\nTest:\nTotal Ratings: 25008\nUnique Users: 943\nUnique Items: 1532\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Train:\n",
    "Total Ratings: {train_total}\n",
    "Unique Users: {train_users}\n",
    "Unique Items: {train_items}\n",
    "\n",
    "Test:\n",
    "Total Ratings: {test_total}\n",
    "Unique Users: {test_users}\n",
    "Unique Items: {test_items}\n",
    "\"\"\".format(\n",
    "    train_total=len(train),\n",
    "    train_users=len(train['userID'].unique()),\n",
    "    train_items=len(train['itemID'].unique()),\n",
    "    test_total=len(test),\n",
    "    test_users=len(test['userID'].unique()),\n",
    "    test_items=len(test['itemID'].unique()),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Train the FBT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Instantiate the SAR algorithm and set the index\n",
    "\n",
    "We will leverage the single node implementation of SAR and specify the column names to match our dataset. Since we don't need timestamps, we will not use it and turn off timestamp related flags. (`timedecay_formula = False`) We will also switch `normalize = False` which is associated to renormalizing ratings predictions obtained form user affinity computation to an appropriate scale.\n",
    "\n",
    "Other options are specified to control the behavior of the algorithm as described in the [deep dive notebook](../02_model_collaborative_filtering/sar_deep_dive.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "\n",
    "model = SAR(\n",
    "    col_user=\"userID\",\n",
    "    col_item=\"itemID\",\n",
    "    col_rating=\"rating\",\n",
    "    similarity_type=\"jaccard\", \n",
    "    time_decay_coefficient=30, \n",
    "    timedecay_formula=False,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the FBT model on our training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "To train (fit) a model, FBT computes an item-to-item ***co-occurence matrix***. Co-occurence represents the number of times two items appear together for any given user. With SAR, we have the option to rescale the co-occurence matrix to an ***item similarity matrix*** by a given metric (Jaccard similarity for example). We will not do this with our vanilla FBT example. \n",
    "\n",
    "SAR also computes an ***affinity matrix*** to capture the strength of the relationship between each user and each item. Affinity is driven by different types (like *rating* or *viewing* a movie). Since for our example, any interaction is Boolean, the affinity matrix is identity.\n",
    "\n",
    "Recommendations are achieved by computing a cooccurrence score per user and course and filtering the ***top-k*** results for each user in the `recommend_k_items` function seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-07 21:11:00,089 INFO     Collecting user affinity matrix\n",
      "2021-05-07 21:11:00,091 INFO     De-duplicating the user-item counts\n",
      "2021-05-07 21:11:00,097 INFO     Creating index columns\n",
      "2021-05-07 21:11:00,183 INFO     Building user affinity sparse matrix\n",
      "2021-05-07 21:11:00,188 INFO     Calculating item co-occurrence\n",
      "2021-05-07 21:11:00,331 INFO     Calculating item similarity\n",
      "2021-05-07 21:11:00,332 INFO     Using jaccard based similarity\n",
      "2021-05-07 21:11:00,361 INFO     Done training\n",
      "Took 0.27520909799204674 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit(train)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-07 21:11:53,158 INFO     Calculating recommendation scores\n",
      "2021-05-07 21:11:53,183 INFO     Removing seen items\n",
      "Took 0.05055569099204149 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "    top_k = model.recommend_k_items(test, remove_seen=True)\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0       1      69   37.659954\n",
       "1       1     423   37.453304\n",
       "2       1     403   37.092041\n",
       "3       1     238   36.293091\n",
       "4       1     568   36.273216"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>69</td>\n      <td>37.659954</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>423</td>\n      <td>37.453304</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>403</td>\n      <td>37.092041</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>238</td>\n      <td>36.293091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>568</td>\n      <td>36.273216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "top_k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      userID  itemID  prediction                          itemTitle\n9420     943      82   26.735399               Jurassic Park (1993)\n9421     943     403   26.123484                      Batman (1989)\n9422     943     568   26.019278                       Speed (1994)\n9423     943     423   25.228394  E.T. the Extra-Terrestrial (1982)\n9424     943     393   25.134943              Mrs. Doubtfire (1993)\n9425     943      11   24.651745               Seven (Se7en) (1995)\n9426     943      89   24.606472                Blade Runner (1982)\n9427     943      71   24.545057              Lion King, The (1994)\n9428     943     202   24.437031               Groundhog Day (1993)\n9429     943      95   24.141525                     Aladdin (1992)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>prediction</th>\n      <th>itemTitle</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9420</th>\n      <td>943</td>\n      <td>82</td>\n      <td>26.735399</td>\n      <td>Jurassic Park (1993)</td>\n    </tr>\n    <tr>\n      <th>9421</th>\n      <td>943</td>\n      <td>403</td>\n      <td>26.123484</td>\n      <td>Batman (1989)</td>\n    </tr>\n    <tr>\n      <th>9422</th>\n      <td>943</td>\n      <td>568</td>\n      <td>26.019278</td>\n      <td>Speed (1994)</td>\n    </tr>\n    <tr>\n      <th>9423</th>\n      <td>943</td>\n      <td>423</td>\n      <td>25.228394</td>\n      <td>E.T. the Extra-Terrestrial (1982)</td>\n    </tr>\n    <tr>\n      <th>9424</th>\n      <td>943</td>\n      <td>393</td>\n      <td>25.134943</td>\n      <td>Mrs. Doubtfire (1993)</td>\n    </tr>\n    <tr>\n      <th>9425</th>\n      <td>943</td>\n      <td>11</td>\n      <td>24.651745</td>\n      <td>Seven (Se7en) (1995)</td>\n    </tr>\n    <tr>\n      <th>9426</th>\n      <td>943</td>\n      <td>89</td>\n      <td>24.606472</td>\n      <td>Blade Runner (1982)</td>\n    </tr>\n    <tr>\n      <th>9427</th>\n      <td>943</td>\n      <td>71</td>\n      <td>24.545057</td>\n      <td>Lion King, The (1994)</td>\n    </tr>\n    <tr>\n      <th>9428</th>\n      <td>943</td>\n      <td>202</td>\n      <td>24.437031</td>\n      <td>Groundhog Day (1993)</td>\n    </tr>\n    <tr>\n      <th>9429</th>\n      <td>943</td>\n      <td>95</td>\n      <td>24.141525</td>\n      <td>Aladdin (1992)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "top_k_with_titles = (\n",
    "    top_k.join(data[['itemID', 'Title']].drop_duplicates().set_index('itemID'), \n",
    "               on='itemID', \n",
    "               how='inner')\n",
    "         .sort_values(by=['userID', 'prediction'], ascending=False)\n",
    "         .rename(columns={'Title': 'itemTitle'})\n",
    ")\n",
    "        \n",
    "display(top_k_with_titles.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluate how well FBT performs\n",
    "\n",
    "We evaluate how well SAR performs for a few common ranking metrics provided in the `python_evaluation` module in reco_utils. We will consider the Mean Average Precision (MAP), Normalized Discounted Cumalative Gain (NDCG), Precision, and Recall for the top-k items per user we computed with SAR. User, item and rating column names are specified in each evaluation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09602432512049103"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "eval_map = map_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3507570877288949"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "eval_ndcg = ndcg_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_precision = precision_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_recall = recall_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_rmse = rmse(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_mae = mae(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model:\t\nTop K:\t10\nMAP:\t0.096024\nNDCG:\t0.350757\nPrecision@K:\t0.308271\nRecall@K:\t0.169100\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\t\",\n",
    "      \"Top K:\\t%d\" % TOP_K,\n",
    "      \"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-07 18:06:43,193 INFO     Calculating recommendation scores\n",
      "2021-05-07 18:06:43,193 INFO     Removing seen items\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   userID  itemID                           Title  prediction\n0     876     604    It Happened One Night (1934)         NaN\n1     876     187  Godfather: Part II, The (1974)         NaN\n2     876     523           Cool Hand Luke (1967)         NaN\n3     876     878           That Darn Cat! (1997)         NaN\n4     876     286     English Patient, The (1996)         NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>Title</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>876</td>\n      <td>604</td>\n      <td>It Happened One Night (1934)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>876</td>\n      <td>187</td>\n      <td>Godfather: Part II, The (1974)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>876</td>\n      <td>523</td>\n      <td>Cool Hand Luke (1967)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>876</td>\n      <td>878</td>\n      <td>That Darn Cat! (1997)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>876</td>\n      <td>286</td>\n      <td>English Patient, The (1996)</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Now let's look at the results for a specific user\n",
    "user_id = 876\n",
    "\n",
    "ground_truth = test[test['userID']==user_id].sort_values(by='rating', ascending=False)[:TOP_K]\n",
    "prediction = model.recommend_k_items(pd.DataFrame(dict(userID=[user_id])), remove_seen=True) \n",
    "test_user_movie_watched_prediction = (\n",
    "    pd.merge(ground_truth, prediction, on=['userID', 'itemID'], how='left')\n",
    "      .drop(columns=['rating'])\n",
    ")\n",
    "display(test_user_movie_watched_prediction.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that one of the movies from the test set was recovered by the model's top-k recommendations, however the others were not. Offline evaluations are difficult as they can only use what was seen previously in the test set and may not represent the user's actual preferences across the entire set of items. Adjustments to how the data is split, algorithm is used and hyper-parameters can improve the results here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "map",
       "data": 0.0700594112753097,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "map",
       "data": true,
       "display": false
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "ndcg",
       "data": 0.30469469553920064,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "ndcg",
       "data": true,
       "display": false
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "precision",
       "data": 0.26521739130434785,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "precision",
       "data": true,
       "display": false
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "recall",
       "data": 0.1250000517480203,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "recall",
       "data": true,
       "display": false
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "train_time",
       "data": 0.2522684158757329,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "train_time",
       "data": true,
       "display": false
      }
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/scrapbook.scrap.json+json": {
       "name": "test_time",
       "data": 0.201675103046,
       "encoder": "json",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "name": "test_time",
       "data": true,
       "display": false
      }
     }
    }
   ],
   "source": [
    "# Record results with papermill for tests - ignore this cell\n",
    "sb.glue(\"map\", eval_map)\n",
    "sb.glue(\"ndcg\", eval_ndcg)\n",
    "sb.glue(\"precision\", eval_precision)\n",
    "sb.glue(\"recall\", eval_recall)\n",
    "sb.glue(\"train_time\", train_time.interval)\n",
    "sb.glue(\"test_time\", test_time.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "python3611jvsc74a57bd098169291db0c76b5a29ac985497b93ea6e9ffb789b0c6c3e8a1bf753f6a69f0f",
   "display_name": "Python 3.6.11 64-bit ('reco_base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}